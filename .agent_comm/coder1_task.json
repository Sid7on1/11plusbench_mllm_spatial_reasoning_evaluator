{
  "agent_id": "coder1",
  "task_id": "task_9",
  "files": [
    {
      "filename": "data_loader.py",
      "purpose": "Handles loading and preprocessing of 11PLUS-BENCH dataset with cognitive annotations",
      "priority": "high",
      "dependencies": [
        "pandas",
        "PIL",
        "jsonlines",
        "datasets"
      ],
      "key_functions": [
        "load_11plus_data",
        "parse_cognitive_annotations",
        "create_dataloader",
        "validate_annotations",
        "handle_image_formats"
      ],
      "estimated_lines": 350,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "11PlusBench_MLLM_Spatial_Reasoning_Evaluator",
    "project_type": "computer_vision",
    "description": "A comprehensive evaluation framework for assessing multimodal large language models' spatial reasoning capabilities against human cognitive profiles using the 11PLUS-BENCH benchmark. The project implements fine-grained cognitive feature analysis, predictive modeling of correctness, and parallel human-model comparison across three core spatial capabilities: Spatial Relation and Orientation, Spatial Visualization, and Flexibility of Closure.",
    "key_algorithms": [
      "RandomForest_classification",
      "Linear_regression_analysis",
      "SHAP_feature_importance",
      "Cognitive_feature_annotation_pipeline",
      "Cross_validation_evaluation"
    ],
    "main_libraries": [
      "torch",
      "transformers",
      "sklearn",
      "pandas",
      "numpy",
      "matplotlib",
      "seaborn",
      "shap",
      "PIL",
      "opencv-python",
      "tqdm",
      "jsonlines",
      "datasets",
      "wandb"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.20068v1_11Plus-Bench-Demystifying-Multimodal-LLM-Spatial-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\n11P LUS-BENCH : Demystifying Multimodal LLM\nSpatial Reasoning with Cognitive-Inspired Analysis\nChengzu Li[MSR\u2217,LTL]Wenshan Wu[MSR]Huanyu Zhang[MSR*,CAS]\nQingtao Li[MSR]Zeyu Gao[ONC]Yan Xia[MSR]\nJos\u00e9 Hern\u00e1ndez-Orallo[CFI,VAL]Ivan Vuli \u00b4c[LTL]Furu Wei[MSR]\nhttps://aka.ms/GeneralAI\n[MSR]Microsoft Research[LTL]Language Technology Lab, University of Cambridge\n[CAS]Institute of Automation, Chinese Academy of Sciences\n[ONC]Department of Oncology, University of Cambridge\n[CFI]Leverhulme Centre for the Future of Intelligence, University of Cambridge\n[VAL]VRAIN, Universitat Polit\u00e8cnica de Val\u00e8ncia\nAbstract\nFor human cognitive process, spatial reasoning and perception are closely entan-\ngled, yet the nature of this interplay remains underexplored in the evaluation of\nmultimodal large language models (MLLMs). While recent MLLM advancements\nshow impressive performance on reasoning, their capacity for human-like spatial\ncognition remains an open question. In this work, we introduce a systematic\nevaluation framework to assess the spatial reasoning abilities of state-of-the-art\nMLLMs relative to human performance. Central to our work is 11P LUS-BENCH , a\nhigh-quality benchmark derived from realistic standardized spatial aptitude tests.\n11P LUS-BENCH also features fine-grained expert annotations of both perceptual\ncomplexity and reasoning process, enabling detailed instance-level analysis of\nmodel behavior. Through extensive experiments across 14 MLLMs and human\nevaluation, we find that current MLLMs exhibit early signs of spatial cognition.\nDespite a large performance gap compared to humans, MLLMs\u2019 cognitive pro-\nfiles resemble those of humans in that cognitive effort correlates strongly with\nreasoning-related complexity. However, instance-level performance in MLLMs\nremains largely random, whereas human correctness is highly predictable and\nshaped by abstract pattern complexity. These findings highlight both emerging\ncapabilities and limitations in current MLLMs\u2019 spatial reasoning capabilities and\nprovide actionable insights for advancing model design.\n1 Introduction\nMany achievements of Large Language Models (LLMs) [ 5,52,1] and their multimodal variants\n(MLLMs) [ 28,58,19] are largely concentrated in domains where reasoning can be framed through\nsymbolic sequence processing, including code generation [ 2,36], mathematical problem solving\n[43,68,69], and question answering [ 76,25,40,80]. Human intelligence goes beyond symbolic\nprocessing. It relies heavily on perceptual intuition and mental imagery to simulate hypothetical\nscenarios via object-based imagery (e.g., of shapes) and spatial imagery (e.g., of locations) [ 46,34],\nwhich is still underexplored with MLLMs [ 39,73]. Spatial reasoning, also referred to spatial\nintelligence in cognitive science, encompasses all thinking about spatial content: object shape or\nlocation, and manipulating, imagining, or inferring relationships between objects in space [ 47].\n\u2217Work done during internship at Microsoft Research.arXiv:2508.20068v1  [cs.CL]  27 Aug 2025\n\n--- Page 2 ---\nCognitive Feature Annotation\nResponse\nTime\nPattern\nPerception\nGeneral\nReasoning\nSpatial CapabilitiesPredictive Power Cognitive Profile Analysis\nSpatial\nCapabilities\nSpatial Relation \nand Orientation\nSpatial\nVisualization Flexibility \nof Closure\nHuman\nModel\nI think this \nquestion is \neasy for me. \n   RandomPredictive\nI have 75% \nprobability \nto get this right. \nHuman Evaluator \nAverage Correctness\n Model Response \nCorrectnessCognitive Effort \nResponse TimeResponse Correctness\nLinear Regression \nCoefficient Analysis\nShap Value\nTree Analysis\nCognitive Effort\nResponse/Thinking LengthResponse Correctness \nReasoning-Relevant Features\nMost Impactful\nPerceptual Features\nand Manipulation\nMost Impactful\nReasoning-Relevant Features\nMost ImpactfulPerceptual Features\nand Spatial Relationship \nUnderstanding More Impactful\n3D - 2D view\n3D shape rotation\n2D shape rotation / reflectionShape combination\nShape completionHidden shape\nPaper foldingBuilding blocks\nDice the cube Cube and nets\nFigure 1: Overview of evaluation framework with 11P LUS-BENCH , including fine-grained\nannotations of cognitive features across diverse tasks targeting three core spatial capabilities. These\nannotations enable predictive modeling of correctness for both humans and MLLMs, followed by\ncognitive profile analysis to identify key features that influence accuracy and cognitive load.\nCarroll\u2019s Three-Stratum Theory of Intelligence [ 8,9] places Visualization andSpatial Relations as\ncore narrow abilities within the general spatial intelligence domain (Gv), contributing to general\nintelligence ( g) as evidenced by empirical research [ 15]. Spatial reasoning is crucial for success\nin STEM fields, visuospatial memory, navigation, and mechanical reasoning [ 23,66,21,37,83].\nDespite its fundamental importance to human intelligence, spatial reasoning remains a relatively\nunderexplored area in the evaluation of artificial intelligence.\nExisting work evaluating MLLM spatial reasoning has largely relied on aggregate metrics such as\noverall or task-wise accuracy [ 56,61,72], which offers only a coarse view of model ability. These\nholistic evaluations often conflate distinct cognitive processes, such as perception, symbolic reasoning,\nand spatial inference [ 82], limiting interpretability and obscuring a model\u2019s true capabilities in spatial\nreasoning. Consequently, pinpointing specific skill deficits in current systems from aggregated\nmetrics is challenging, leading to potential misattributions (e.g., mistaking perceptual failures for\nreasoning deficits [ 11,12]) and hindering clear improvement pathways for MLLM spatial cognition.\nFurthermore, despite referencing human cognitive tests as testbed, comparisons between human\ncognition and model behavior in existing work remain relatively shallow [ 72,71,79], failing to\nspecifically highlight current MLLM systems\u2019 deficiencies compared to human capabilities.\nTo address these gaps, we ask: Do current MLLMs engage in spatial reasoning in a manner aligned\nwith human cognition? We refer to the strategies and capabilities of perception, interpretation, and\nreasoning in spatial contexts as the model\u2019s cognitive profile, and we aim to facilitate a parallel\ncomparison of these cognitive profiles between humans and MLLMs.\nTo this end, we present this evaluation framework centered around 11P LUS-BENCH , a newly-\nintroduced high-quality benchmark grounded in standardized spatial aptitude tests used in human\ncognitive assessments [ 64,27]. This design isolates spatial reasoning from confounding factors such\nas commonsense knowledge or numerical ability. Unlike traditional benchmarks that emphasize\naggregate accuracy, 11P LUS-BENCH supports instance-level comparisons between the correctness\nof model responses and the perceived difficulty of human behaviors. It features fine-grained expert\nannotations ofcognitive features , capturing both visual pattern complexity (perceptual load) and\nreasoning process (inference difficulty), allowing us to investigate and disentangle different factors\nthat influence system behavior. To compare with human performance, we conduct human evaluations\n2\n\n--- Page 3 ---\nwith three participants and use response time as a proxy for cognitive load [ 4,35]. Our annotations\nexhibit high inter-annotator agreement and strong predictive power for participant response time with\nannotated cognitive features, validating the benchmark\u2019s quality and interpretability. 11P LUS-BENCH\nalso minimizes contamination concerns by collecting expert annotations for data with no golden\nanswers (over 50%) and holding out a test split composed of problems sourced from commercial test\nproviders that are not publicly available.\nExperimental results across 14 state-of-the-art MLLMs reveal a substantial performance gap between\nmodels and humans, emphasizing the current limitations of MLLMs in spatial reasoning. While\nadvanced proprietary MLLMs show early signs of spatial reasoning ability, their instance-level\nperformance remains random and poorly predictable with human-inspired cognitive features above.\nFurther analysis uncovers both convergence and divergence in cognitive profiles. Reasoning-related\ncomplexity correlates strongly with cognitive load, measured by response time in humans and token\ncounts of response for MLLMs. However, model performance is more sensitive to understanding\nlow-level visual cues such as image resolution and spatial relations, whereas human accuracy is\nprimarily influenced by abstract pattern complexity. This blend of similarity and divergence reveals\nboth the emergence of spatial reasoning capabilities in MLLMs and their current deficiencies. Unlike\nhumans, whose spatial reasoning is structured, MLLMs often lack the robustness and compositional\nunderstanding necessary for consistent, human-like spatial cognition.\n2 Related Work\nSpatial Aptitude Test in Cognitive Science Human spatial ability includes intrinsic object-centred\nskills (e.g., mental rotation, paper -folding) and extrinsic environment -centred skills (e.g., perspective\ntaking, navigation) [ 26]. Classic experimental work on mental rotation by Shepard & Metzler [59]\nand Cooper [14] frames rotation as a continuous internal transformation. Factor -analytic syntheses\nlater showed that rotation loads on a separable spatial factor distinct from verbal or numerical\nreasoning [ 44,41,8]. Perspective -taking studies, notably Hegarty & Waller [24], demonstrated a\ndouble dissociation from mental rotation, motivating multi -dimensional test batteries such as the\nVandenberg\u2013Kuse Mental Rotation Test, Paper -Folding and Spatial Orientation tests [ 17]. Training\nmeta -analyses confirm that spatial skills are malleable and transfer to STEM success [ 10,64].\nNeuropsychological reviews link these competencies to parietal\u2013frontal circuits and hippocampal\nplace/grid coding, underscoring their foundational role in cognition [ 7,29]. Together, these findings\nprovide both theoretical structure and validated psychometrics that any AI-oriented spatial benchmark\nshould respect.\nSpatial Cognition with MLLMs Early multimodal benchmarks such as CLEVR [ 31] and NLVR 2\n[62] introduced synthetic and natural -image tasks that hinge on recognising static binary relations\n(e.g., left of, behind ). Subsequent datasets, e.g. SpatialSense [ 75], Spatial -MM [ 60], and Comsa\n& Narayanan\u2019s preposition suite [ 13], tightened the focus on fine -grained relational semantics. Yet\nperformance plateaus suggest that current MLLMs still rely on language priors rather than genuine\ngeometric reasoning [ 67,73]. Dynamic extensions (CLEVRER [ 77], TopViewRS [ 38], VSI -Bench\n[74]) add temporal sequences, but typically restrict transformations to planar translation or simple\ncollisions, leaving rotation, reflection, and multi -step composite reasoning under -explored. Holistic\ntest batteries such as MindtheGap [61] and SAT [ 57] broaden the coverage by emulating psychometric\ntasks. Despite the breadth, analyses remain largely descriptive, reporting that \u201cMLLMs fail\u201d without\nisolating why(e.g., frame -of-reference confusion, object -correspondence errors) or benchmarking\nagainst human baselines [ 56]. Our benchmark, 11P LUS-BENCH , adopts a cognitive science\u2013informed\ntaxonomy and includes human performance statistics for each item, enabling detailed, parallel analysis\nof model and human cognitive profiles.\n3 11P LUS-BENCH Benchmark\n3.1 Collection of Tasks\nSpatial Capabilities. Human cognitive development involves several key capabilities that collectively\nform spatial intelligence. Psychometric research has identified and quantified these through stan-\n3\n\n--- Page 4 ---\ndardized tests, capturing dimensions such as Spatial Relation and Orientation, Spatial Visualization,\nFlexibility of Closure, Perceptual Speed, Spatial Memory, and more [59, 41, 7, 78, 32, 70, 16].\nHowever, not all these capabilities are equally relevant for evaluating current MLLMs, given funda-\nmental differences in reasoning mechanisms between human cognition and machine learning models.\nFor instance, perceptual speed is less critical for current MLLM paradigms, which do not process\ninformation in real-time like humans. Similarly, factors like spatial memory [ 7,16] (e.g., recalling\nroutes or locations over time) or kinesthetic spatial reasoning (understanding space through bodily\nmovement) [ 54,55] may not directly translate to current MLLM architectures which primarily operate\non simulated static multimodal inputs. Therefore, we select three representative spatial capabilities:\n\u2022Spatial Relation and Orientation ( SRO ): Involves understanding relationships between objects\nin space, including distance, direction, and position [ 48,78]. It is essential for tasks requiring\nrecognition of spatial configurations and interrelations.\n\u2022Spatial Visualization (SV): Refers to the ability to mentally manipulate and transform spatial\ninformation [ 45,59]. This is important for tasks involving mental rotation, pattern recognition, and\nimagining as well as manipulating objects or scenes.\n\u2022Flexibility of Closure (FoC): Pertains to the ability to perceive and mentally complete incom-\nplete patterns or shapes [ 78]. This cognitive ability is crucial for solving problems that require\nidentification of missing or occluded elements.\nTask Selection. We utilize well-established psychometric tests corresponding to the selected capa-\nbilities [ 22,42,30,53,20,65]. These tests are widely acknowledged and developed in cognitive\nscience, ensuring a fair and parallel comparison between AI systems and humans. Because most\npsychometric tests use diagrams and structured questions as multimodal input, they also allow for\ncontrolled experiments in terms of task complexity while controlling other irrelevant factors to spatial\nintelligence, such as entity recognition in real-world images. Table 1 presents the correspondence\nbetween tasks and capabilities, and Figure 1 provides concrete examples. See Appendix A for detailed\ndefinitions of each task.\n3.2 Collection of Cognitive Features\nAnswering spatial cognition questions not only requires spatial reasoning but also depends on visual\nperception and general reasoning performance. These factors influence the probability of a correct\nresponse from both humans and machines but do not directly measure spatial reasoning. For a\nfine-grained explainable investigation, we collect performance-relevant cognitive features as follows:\nVisual Perception. More complex patterns require greater cognitive load for humans to perceive and\nanalyze. For both the question and options, we quantify pattern complexity as the number of atomic\ncomponents in the patterns as key features, defined by how humans perceive and analyze patterns\n(details on the objective definition of \u2018key features\u2019 can be found in Appendix A).\nGeneral Reasoning. Longer reasoning chains indicate greater question complexity and a higher\nlikelihood of error [ 18,33]. Transitions among reasoning types, such as logical deduction and pattern\nrecognition, add extra cognitive load. These features are distinct from intrinsic spatial cognition\nbut influence reasoning time or response correctness. Variations in these features are subjectively\nprofound, as different individuals may adopt different reasoning chains, especially for more complex\nquestions. To account for this subjective variation, we annotate the general reasoning process by\nrequiring human annotators to choose from four predefined categories of atomic operations: Pattern\nMatching ,Spatial Relation Analysis ,Spatial Manipulation , and Logical Deduction , each comprising\na set of specific operations with details in Appendix A.\nIn addition to these cognitive features, bounding boxes of question and option patterns are also\ncollected in pixel coordinates.\n3.3 Benchmark Construction\nTo facilitate the evaluation framework, we construct the 11P LUS-BENCH with realistic cognitive\nscience test targeted for teenagers aged 11 or above ( 11P LUS). We compile the public portion of\n4\n\n--- Page 5 ---\nour benchmark by crawling the web using carefully chosen spatial reasoning keywords. A rule-\nbased filtering pipeline is then introduced to discard irrelevant, ambiguous, or non-spatial reasoning\nsamples, ensuring data quality and relevance. Implementation details are provided in Appendix A.\nConcurrently, the private portion of our benchmark is sourced by purchasing materials from official\ntest centers. This dual approach, combining newly annotated public data with proprietary test-center\nmaterials, creates a robust and professional dataset that captures a broad spectrum of spatial cognition\nchallenges while ensuring data quality and contamination control for model evaluation.\nAll annotations were performed by three human experts, who are postgraduate-level or higher with\nmathematical or engineering backgrounds. Annotators were trained using standardized guidelines\nto ensure consistency and reliability across the dataset. They annotated the entire public set and\nan additional 100 samples drawn from the private set, creating a diverse and robust foundation for\nevaluating spatial reasoning. Data examples deemed low-quality, without a correct answer, or not\nbelonging to spatial cognition were manually filtered and discarded. By combining thorough filtering\nwith expert human annotation, we ensure the benchmark reflects genuine spatial cognition challenges\nand minimizes errors.\nReasoning Steps\nPattern Complexity \nExamples\nReferrable pattern: \nnumber of symbols (3)\nQuestion Options\nConsistent Num 905 4,069\nTotal Num 915 4,097\nPercentage 98.9% 99.3%\n(a) Expert Annotation Correctness\n(b) Subjective Annotation Agreement(c) Objective Annotation Consistency\n2D abstract pattern: \nnumber of lines (8)\n3D abstract pattern: \nnumber of surface (7)\nFigure 2: Quality analysis of expert data collection. Expert\nannotations achieve high accuracy on private data with golden\nanswers and exhibit strong agreement across both subjective\nand objective annotation fields.Benchmark Quality Analysis\nThe fine-grained annotated bench-\nmark contains 824 data points in\nthe public set and 91 data points\nin the private set after filtering, all\nannotated by 3 domain experts. The\nannotations exhibit strong internal\nconsistency and correctness, un-\nderscoring the high quality of the\ndataset, as shown in Figure 2. The\nannotated answers achieve 94.5%\naccuracy on private set against\ngold-standard labels. For subjective\nfields such as Reasoning Steps, we\nobserve a high level of annotator\nagreement, with Pearson correlation\ncoefficients typically around or\nabove 0.8. The objective pattern\ncomplexity for both questions and\noptions shows perfect agreement\namong annotators, with numbers strictly aligned. Appendix A provides more information about our\nbenchmark.\nData Highlights Here are the key highlights of 11P LUS-BENCH :\n\u2022More Realistic Data :11P LUS-BENCH contains two separate data splits (public with 824 examples\n& private with 91 examples), all derived from realistic 11Plus spatial aptitude test. The public\nset was crawled from the web, while the private set was purchased from test centers and involves\ncopyrights and intellectual properties.\n\u2022Lower Risk of Data Contamination : With experts annotating over 50% data with no golden answer\navailable and withholding the private set due to intellectual property considerations, 11P LUS-\nBENCH significantly lowers the risk of data contamination when evaluating model performance.\n\u2022Richer Cognitive Features : In addition to the golden answer, 11P LUS-BENCH provides richer\nfields including not only bounding boxes for patterns but also visual perception complexity, general\nreasoning process as cognitive annotation.\n4 Experiments and Results\n4.1 Experimental Setups\nModels To comprehensively assess the spatial cognition capabilities of contemporary Multimodal\nLarge Language Models (MLLMs), we selected a diverse suite of 14 models. This selection encom-\n5\n\n--- Page 6 ---\npasses both open-sourced and close-sourced architectures, varying significantly in their parameter\ncounts and underlying designs. Specifically, we evaluated four open-sourced models: Qwen-VL-2.5\n[3] (with 3B and 7B parameters) and Gemma 3 [ 63] (with 12B and 27B parameters). Complementing\nthese, we included ten close-sourced MLLMs: GPT-4o, GPT 4.1 mini, GPT 4.1 nano, GPT-o1,\nGPT-o3, GPT-o4-mini, GPT4.1, Gemini 2.0 Flash preview, Gemini 2.5 Flash preview and Gemini 2.5\nPro preview [ 28,49\u201351,58,19]. This curated set allows for a broad analysis of how model scale and\naccessibility correlate with performance.\nTask Settings The evaluation methodology extends traditional Visual Question Answering (VQA)\nbenchmarks by also presenting multiple images as options in response to a given question. We\ninvestigate two distinct presentation formats to evaluate the MLLMs\u2019 spatial cognition:\n1.Single Composite Image : In this setup, a single image is presented to the model, as with\nhumans. This image contains both the primary image relevant to the question and all\ncandidate option images arranged spatially. This approach is adopted by previous works in\nbenchmarking the spatial cognition performance of MLLMs [61, 56, 72].\n2.Separate Images with Bounding Box Annotations : The primary image and each option\nimage are cropped from the original images as distinct, separate visual inputs. This allows\nmodels to potentially ground their reasoning more precisely on specific visual elements.\nThe performance of the MLLMs across all tasks is quantified by their accuracy in selecting the correct\noption image that answers the posed question.\nHuman Evaluation Three participants who are not involved in the annotation process are recruited\nin order to assess human performance on 11P LUS-BENCH benchmark, strictly adhering to ethical\nregulations. The examples for human evaluation are uniformly sampled from different tasks, with\nall data being used for specific task if the available examples are less than sampling requirements,\nresulting in 402 examples in total. In addition to collecting participants\u2019 selected answers, we record\ntheresponse time for each human participant to answer the question, measured in seconds, as an\noutcome-driven proxy for overall cognitive load [4, 35].\n4.2 Results\nHuman Performance Human participants achieve accuracies of 72%, 87% and 85% across the 402\nexamples. Of all the examples, 241 of them are answered correctly by all three participants, 115 are\nanswered correctly by two and 46 questions are answered correctly by one or none. Response times\nexhibit moderate correlation among participants, with a Pearson correlation coefficient exceeding 0.4.\nAdditionally, the intraclass correlation coefficient ( ICC 2 = 0 .529) indicates moderate agreement,\nand the average response time is deemed reliable with ICC 2K= 0.771, reflecting good consistency\nacross participants. We also investigate the relationship between response correctness and average\nresponse time, showing an inverse correlation ( Pearson =\u22120.284). This reveals that questions\nwith higher accuracy tend to elicit shorter response times.\nOverall Model Performance We present a comprehensive overview of the performance of all\nevaluated MLLMs in Figure 3(a). This includes a direct comparison of accuracies achieved under\nboth the single composite image and the separate images task settings. The results highlight signifi-\ncant variability in performance, not only between different models but also across the two distinct\nevaluation paradigms. Closed-sourced models generally achieve higher accuracy than open-source\nmodels. Within open-source models, there is no significant performance difference based on model\nsize; all open-sourced models perform comparably to a randomly sampled baseline. Furthermore, we\ninvestigate whether model response length correlates with accuracy, analogous to trends observed\nin human performance. Using Gemini 2.5 Pro which provides token-level counts for both internal\nreasoning (\u201cthinking\u201d) and final response, we measure the Pearson correlation between response\nlength and accuracy. The resulting correlation coefficient is 0.021, indicating no meaningful relation-\nship between the two and suggesting that, unlike in humans, longer responses do not reflect deeper\nor more accurate reasoning in the model. A detailed breakdown of scores per model and per task\ncategory is provided in Table 4 and 5.\n6\n\n--- Page 7 ---\n(a) Overall Model Performance with Different Input Format\n(b) Model Accuracy Correlates with Human Accuracy\nFigure 3: (a)Models perform better with multiple separate images as input compared to a single\nimage. With multiple-image input, most closed-source models pass the significance test (p < 0.05)\nover random guess, whereas still all open-sourced models fail. (b)MLLM performance correlates\nwith human accuracy (0\u20133 correct responses across all participants), achieving higher accuracy on\ninstances rated as easier by human evaluators.\nCritique of Single Composite Image Evaluation Our findings indicate a notable discrepancy\nin model performance between the two evaluation settings in advanced models. Specifically, the\nsingle composite image approach consistently yielded lower accuracies by 4% on average across\nGPT series models compared to the separate images setting. Most closed-source models significantly\noutperformed a random baseline ( p < 0.05) when using separate images, whereas only GPT o3 and\no4-mini showed significant difference from the baseline with a single composite image input. This\nobservation suggests that the challenge in the single image setup may stem more from the complexities\nof parsing cluttered visual components and segregating distinct conceptual entities, rather than purely\nfrom a deficiency in spatial reasoning. Consequently, we posit that previous benchmarks employing\nsolely this composite image methodology do not accurately reflect the intrinsic spatial cognition\ncapabilities of current MLLMs. Therefore, we only discuss evaluation results with separate images\nas input in the following sections.\nModels are more likely to success on instances that humans perceive as easier. We investigate\nwhether MLLM performance is essentially random across different complexity levels reflected by\nhuman performance. Figure 3(b) plots model accuracy against average human accuracy for the same\nset of examples, revealing a general upward trend: models tend to perform better on instances that\nhumans also find easier, indicated by positive slopes. This correlation, supported by statistically\nsignificant tests against a random baseline, suggests that current MLLMs do exhibit early signs of\nspatial cognition. While their reasoning remains limited, the non-random variation in performance\nacross difficulty levels justifies the presence of spatial cognition in these models.\n4.3 Discussion and Analysis\nBuilding on our high-level performance analysis, we investigate whether instance-wise correctness\ncan be predicted from relevant features. This is important for assessing the reliability of MLLMs:\nif consistent patterns exist, model correctness can be anticipated, enabling safer and more robust\ndeployment [81]. Comparison with humans further reveals how closely MLLMs mirror human-like\nspatial reasoning and help to guide model development.\n7\n\n--- Page 8 ---\nRandom Forest Linear Regression\nHuman Aggregated Results\nCorrectness: \n3/3: 241 / 402\n2/3: 115 / 402\n1/3 or None: 46 / 402Accuracy:    0.542\nF1 Score:    0.279Train Loss:     0.263 +- 0.028\nTest Loss:      0.144 +- 4.746\nover Response Length (# Tokens)\nGemini 2.5 Pro\nCorrectness: \n118 / 402 = 0.294\nAccuracy:    0.647\nF1 Score:     0.629Train Loss:     0.193 +- 0.015\nTest Loss:      -17.693 +- 576.340over Response Correctness over Response Time (s) over Response Correctness\nAbsolute Regression CoefficientsFigure 4: Cognitive profile analysis using SHAP values for correctness prediction and linear regression\ncoefficients for cognitive load, comparing humans and MLLMs. More results in Figures 6 and 7.\nAnalysis Setups To explore how well perceptual and reasoning features can explain behavior\n(cognitive profile), we use machine learning classifiers (random forest) to predict instance-level\ncorrectness for both humans and MLLMs. To address label imbalance, class weights are adjusted\ninversely to class frequencies in the input data when training the classifier. We consider two classifi-\ncation settings: binary classification (correct vs. incorrect) and four-class classification (0\u20133 correct\nresponses across participants). To further analyze cognitive effort, we apply linear regression to\npredict human response time and MLLM token counts including thinking using the same set of\nfeatures. The cognitive-related input features are introduced as follows, encompassing both perceptual\nand reasoning-related dimensions.\nFor visual perception, we include three features: the pattern complexity of both the question and the\nanswer options, as well as the image resolution. Image resolution can impact perceptual recognition,\nwith lower fidelity obscuring visual structure, so we discretize resolution into three bins (low, medium,\nhigh) to reflect practical perceptual clarity. For general reasoning, we extract four features representing\nthe number of reasoning steps required for each category of atomic operations: Pattern Matching ,\nSpatial Relation Analysis ,Spatial Manipulation , and Logical Deduction . To ensure a stable signal\nfrom human, in addition to the correctness of individual human participant, we aggregated responses\nfrom three evaluators, as individual responses may be subject to idiosyncratic noise preventing\nreliable modeling of human cognitive profiles, while models are largely deterministic.\nHuman correctness is predictable while MLLMs exhibit near-random instance-level behavior.\nWe train the classifiers over the set of examples for human evaluation for fair comparison between\nhuman and models using 5-fold cross-validation. Our goal is not to maximize classification accuracy,\nbut to identify the presence or absence of structured cognitive profiles. To mitigate the effects of\nsevere data imbalance and limited samples per fold due to high human accuracy and low model\naccuracy, we aggregate predictions across folds for more stable metric estimation. Human correctness\nof individual participants is highly predictable with Random Forest, reaching weighted F1 scores\nof 0.631, 0.821 and 0.799 ( p < 0.0002 ) and AUC score of 0.579, 0.643 and 0.621. In the more\ngranular four-class setting (aggregated human correctness), the classifier still performs above chance\n(F1 = 0.279 vs. 0.192, p < 0.05), reinforcing the presence of systematic cognitive behavior. In\ncontrast, classifiers trained on MLLM outputs fail to detect consistent correctness patterns. As shown\nin Figure 7, weighted F1 scores and AUC scores remain lower than human participants across most\nmodel variants, with no significant improvement over random baselines ( p > 0.01). These results\nsuggest that human responses are governed by predictable cognitive strategies, while current MLLMs\nlack the internal structure for reliable spatial reasoning at instance level.\nPattern complexity drives human correctness, while reasoning features govern cognitive effort.\nTo understand which features contribute most to human success, we apply SHAP analysis to the\n8\n\n--- Page 9 ---\ntrained classifiers. As shown in Figures 4 and 6, Pattern Complexity (especially in answer options)\nis the strongest predictor of correctness across all participants. This is followed by the presence of\nSpatial Manipulation , a cognitively demanding reasoning step. We further model human response\ntime, a proxy for cognitive effort, using linear regression on the same features. The model predicts\ntime with average error <1 second ( \u00b14s), and analysis of coefficients shows that reasoning features\n(Spatial Relation Analysis ,Spatial Manipulation ,Logical Deduction ) are the dominant contributors\nto increased response time. Interestingly, Pattern Matching correlates with shorter response times,\npossibly due to heuristic strategies such as visual elimination or rule-of-thumb matching. Together,\nthese results highlight a dual cognitive profile in humans: while perceptual errors (e.g., misreading\ncomplex patterns) drive most mistakes, reasoning complexity governs cognitive effort.\nMLLMs show partial alignment with human profiles, but responses remain sensitive to low-level\nvisual cues. We apply SHAP analysis to the classifiers trained on MLLM correctness (Figure 7) and\nobserve high variability across models, with most failing to reach statistical significance (denoted\ninorange withp > 0.01). Still, some convergence with human cognition emerges. Option Pattern\nComplexity is a shared influential feature across both humans and MLLMs, while features like Image\nResolution andSpatial Relation Analysis are more prominent for certain MLLMs. This suggests that\nwhile models do attend to meaningful patterns, they remain disproportionately influenced by low-level\nvisual cues and spatial relationship understanding. To further investigate MLLM effort, we model\n\u201cthinking length\u201d using linear regression. Here, we find that in addition to reasoning-related features,\nQuestion Pattern Complexity contributes significantly, while Spatial Relation Analysis appears to be\nthe least predictive factor, marking a clear divergence from human profiles. These findings point to\na hybrid picture: while MLLMs exhibit emerging spatial awareness, their instance-level reasoning\nremains noisy and constrained by understanding low-level visual cues, calling for further research.\n5 Conclusion\nThis work introduced a novel framework with 11P LUS-BENCH benchmark for evaluating MLLMs\u2019\nspatial cognition against human cognitive profiles, moving beyond aggregate accuracy with fine-\ngrained analysis. Our findings show that while current MLLMs show early signs of spatial reasoning,\ntheir overall capabilities remain limited with randomness. Human accuracy is consistently shaped by\npattern complexity and reasoning demands, revealing structured and predictive cognitive profiles. In\ncontrast, model behavior is more influenced by understanding low-level visual cues such as image\nresolution and spatial relations, with less predictable and interpretable responses at instance level.\nThese results highlight both emerging capabilities and critical gaps between human and MLLMs\nspatial cognition. We hope our findings and 11P LUS-BENCH benchmark with finegrained cognitive\nfeature annotations serve as a foundation for future research toward closing this gap, enabling the\ndevelopment of MLLMs with more robust, human-aligned spatial capabilities.\nReferences\n[1]Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E.,\nBailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.\nURL https://doi.org/10.48550/arXiv.2305.10403 .\n[2]Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai,\nC., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint\narXiv:2108.07732 , 2021.\n[3]Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J.,\net al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025.\n[4]Barrouillet, P., Bernardin, S., Portrat, S., Vergauwe, E., and Camos, V . Time and cognitive load\nin working memory. Journal of Experimental Psychology: Learning, Memory, and Cognition ,\n33(3):570\u2013585, 2007. doi: 10.1037/0278-7393.33.3.570.\n[5]Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T.,\nChild, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I.,\n9\n\n--- Page 10 ---\nand Amodei, D. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.\ncc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .\n[6]Burden, J., V oudouris, K., Burnell, R., Rutar, D., Cheke, L., and Hern\u00e1ndez-Orallo, J.\nInferring capabilities from task performance with bayesian triangulation. arXiv preprint\narXiv:2309.11975 , 2023.\n[7]Burgess, N. Spatial cognition and the brain. Annals of the New York Academy of Sciences , 1124\n(1):77\u201397, 2008. doi: 10.1196/annals.1440.002.\n[8]Carroll, J. B. Human Cognitive Abilities: A Survey of Factor-Analytic Studies . Cambridge Uni-\nversity Press, Cambridge, UK, 1993. ISBN 9780521387125. doi: 10.1017/CBO9780511571312.\n[9]Carroll, J. B. The three-stratum theory of cognitive abilities. In Flanagan, D. P., Genshaft, J. L.,\nand Harrison, P. L. (eds.), Contemporary Intellectual Assessment: Theories, Tests, and Issues ,\npp. 122\u2013130. The Guilford Press, New York, NY , 1997.\n[10] Cheng, Y .-L. and Mix, K. S. Spatial training improves children\u2019s mathematics ability. Journal\nof Cognition and Development , 15(1):2\u201311, 2014. doi: 10.1080/15248372.2012.725186.\n[11] Chollet, F., Knoop, M., Kamradt, G., and Landers, B. Arc prize 2024: Technical report. arXiv\npreprint arXiv:2412.04604 , 2024.\n[12] Chollet, F., Knoop, M., Kamradt, G., Landers, B., and Pinkard, H. Arc-agi-2: A new challenge\nfor frontier ai reasoning systems. arXiv preprint arXiv:2505.11831 , 2025.\n[13] Comsa, I. M. and Narayanan, S. A benchmark for reasoning with spatial prepositions. In Pro-\nceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\npp. 16328\u201316335, 2023.\n[14] Cooper, L. A. Mental rotation of random two-dimensional shapes. Cognitive Psychology , 7(1):\n20\u201343, 1975. doi: 10.1016/0010-0285(75)90003-1.\n[15] Deary, I. J., Penke, L., and Johnson, W. The neuroscience of human intelligence differences.\nNature Reviews Neuroscience , 11:201\u2013211, 2010. doi: 10.1038/nrn2793.\n[16] Ekstrom, A. D. and Hill, P. F. Spatial navigation and memory: A review of the similarities and\ndifferences relevant to brain models and age. Neuron , 111(7):1037\u20131049, 2023.\n[17] Ekstrom, R. B. and Harman, H. H. Manual for Kit of Factor-Referenced Cognitive Tests .\nEducational Testing Service, Princeton, NJ, 1976.\n[18] Garey, M. R. and Johnson, D. S. Computers and intractability , volume 29. wh freeman New\nYork, 2002.\n[19] Gemini. Gemini 2.5: Our most intelligent AI model. March\n2025. URL https://blog.google/technology/google-deepmind/\ngemini-model-thinking-updates-march-2025/ . Accessed: 2025-05-09.\n[20] Gunalp, P., Moossaian, T., and Hegarty, M. Spatial perspective taking: Effects of social,\ndirectional, and interactive cues. Memory & cognition , 47:1031\u20131043, 2019.\n[21] Harris, D. Spatial ability, skills, reasoning or thinking: What does it mean for mathematics?\nIn Leong, Y ., Kaur, B., Choy, B., Yeo, J., and Wong, L. (eds.), Excellence in Mathematics\nEducation: Foundations and Pathway , pp. 219\u2013226. Mathematics Education Research Group of\nAustralasia, 2021. ISBN 9781920846329. URL http://www.merga.net.au . 43rd Annual\nConference of the Mathematics Education Research Group of Australasia 2021 : Excellence\nin Mathematics Education: Foundations & Pathways ; Conference date: 05-07-2021 Through\n08-07-2021.\n[22] Harris, J., Newcombe, N. S., and Hirsh-Pasek, K. A new twist on studying the development of\ndynamic spatial transformations: Mental paper folding in young children. Mind, Brain, and\nEducation , 7(1):49\u201355, 2013.\n10\n\n--- Page 11 ---\n[23] Harvey, T. J. The correlation between mechanical reasoning and spatial ability for first year\nsecondary school boys and girls \u2014 a research note. Journal of Further and Higher Education ,\n9(2):77\u201380, 1985. doi: 10.1080/0309877850090208. URL https://doi.org/10.1080/\n0309877850090208 .\n[24] Hegarty, M. and Waller, D. A dissociation between mental rotation and perspective-taking\nspatial abilities. Intelligence , 32(2):175\u2013191, 2004. doi: 10.1016/j.intell.2003.12.001.\n[25] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.\n[26] Hodgkiss, A., Gilligan, K. A., Tolmie, A. K., Thomas, M. S., and Farran, E. K. Spatial cognition\nand science achievement: The contribution of intrinsic and extrinsic spatial skills from 7 to 11\nyears. British Journal of Educational Psychology , 88(4):675\u2013697, 2018.\n[27] Hodgkiss, A., Gilligan, K. A., Tolmie, A. K., Thomas, M. S. C., and Farran, E. K. Spatial\ncognition and science achievement: The contribution of intrinsic and extrinsic spatial skills\nfrom 7 to 11 years. British Journal of Educational Psychology , 88(4):675\u2013697, 2018. doi:\nhttps://doi.org/10.1111/bjep.12211. URL https://bpspsychub.onlinelibrary.wiley.\ncom/doi/abs/10.1111/bjep.12211 .\n[28] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda,\nA., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 , 2024.\n[29] Husain, M. and Nachev, P. Space and the parietal cortex. Trends in Cognitive Sciences , 11(1):\n30\u201336, 2007. doi: 10.1016/j.tics.2006.10.011.\n[30] Jirout, J. J. and Newcombe, N. S. Building blocks for developing spatial skills: Evidence from\na large, representative us sample. Psychological science , 26(3):302\u2013310, 2015.\n[31] Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R.\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.\n1988\u20131997, 2017.\n[32] Johnson, J. F., Barron, L. G., Carretta, T. R., and Rose, M. R. Predictive validity of spatial\nability and perceptual speed tests for aviator training. The International Journal of Aerospace\nPsychology , 27(3-4):109\u2013120, 2017.\n[33] Johnson-Laird, P. N. Mental models and human reasoning. Proceedings of the National\nAcademy of Sciences , 107(43):18243\u201318250, 2010.\n[34] Kozhevnikov, M., Kosslyn, S., and Shephard, J. Spatial versus object visualizers: A new\ncharacterization of visual cognitive style. Memory & cognition , 33(4):710\u2013726, 2005.\n[35] Kyllonen, P. C. and Zu, J. Use of response time for measuring cognitive ability. Journal\nof Intelligence , 4(4), 2016. ISSN 2079-3200. doi: 10.3390/jintelligence4040014. URL\nhttps://www.mdpi.com/2079-3200/4/4/14 .\n[36] Lai, Y ., Li, C., Wang, Y ., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S.,\nand Yu, T. Ds-1000: A natural and reliable benchmark for data science code generation. In\nInternational Conference on Machine Learning , pp. 18319\u201318345. PMLR, 2023.\n[37] Li, C., Zhang, C., Teufel, S., Doddipatla, R. S., and Stoyanchev, S. Semantic map-based\ngeneration of navigation instructions. In Calzolari, N., Kan, M.-Y ., Hoste, V ., Lenci, A.,\nSakti, S., and Xue, N. (eds.), Proceedings of the 2024 Joint International Conference on\nComputational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pp.\n14628\u201314640, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.\norg/2024.lrec-main.1274/ .\n[38] Li, C., Zhang, C., Zhou, H., Collier, N., Korhonen, A., and Vuli \u00b4c, I. TopViewRS:\nVision-language models as top-view spatial reasoners. In Al-Onaizan, Y ., Bansal, M., and\n11\n\n--- Page 12 ---\nChen, Y .-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Nat-\nural Language Processing , pp. 1786\u20131807, Miami, Florida, USA, November 2024. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.106. URL\nhttps://aclanthology.org/2024.emnlp-main.106/ .\n[39] Li, C., Wu, W., Zhang, H., Xia, Y ., Mao, S., Dong, L., Vuli \u00b4c, I., and Wei, F. Imagine while\nreasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542 ,\n2025.\n[40] Li, C., Zhou, H., Glava\u0161, G., Korhonen, A., and Vuli \u00b4c, I. Large language models are miscal-\nibrated in-context learners. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.),\nFindings of the Association for Computational Linguistics: ACL 2025 , pp. 11575\u201311596,\nVienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-\n256-5. doi: 10.18653/v1/2025.findings-acl.603. URL https://aclanthology.org/2025.\nfindings-acl.603/ .\n[41] Linn, M. C. and Petersen, A. C. Emergence and characterization of sex differences in spatial\nability: A meta-analysis. Child Development , 56(6):1479\u20131498, 1985. doi: 10.2307/1130467.\n[42] Lovett, A. and Forbus, K. Modeling spatial ability in mental rotation and paper-folding. In\nProceedings of the Annual Meeting of the Cognitive Science Society , volume 35, 2013.\n[43] Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley,\nM., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual\ncontexts. In International Conference on Learning Representations (ICLR) , 2024.\n[44] McGee, M. G. Human spatial abilities: Psychometric studies and environmental, genetic,\nhormonal, and neurological influences. Psychological Bulletin , 86(5):889\u2013918, 1979. doi:\n10.1037/0033-2909.86.5.889.\n[45] Michael, W. B., Guilford, J. P., Fruchter, B., and Zimmerman, W. S. The description of\nspatial-visualization abilities. Educational and psychological measurement , 17(2):185\u2013199,\n1957.\n[46] Moulton, S. T. and Kosslyn, S. M. Imagining predictions: mental imagery as mental emulation.\nPhilosophical Transactions of the Royal Society B: Biological Sciences , 364(1521):1273\u20131280,\n2009. doi: 10.1098/rstb.2008.0314.\n[47] Newcombe, N. S. Spatial Cognition . MIT Press, jul 24 2024. https://oecs.mit.edu/pub/or750iar.\n[48] Newcombe, N. S. and Learmonth, A. E. Development of spatial competence. The Cambridge\nhandbook of visuospatial thinking , pp. 213\u2013256, 2005.\n[49] OpenAI. Introducing OpenAI o1. September 2024. URL https://openai.com/o1/ . Ac-\ncessed: 2025-06-08.\n[50] OpenAI. Introducing GPT-4.1 in the API. April 2025. URL https://openai.com/index/\ngpt-4-1/ . Accessed: 2025-06-08.\n[51] OpenAI. Introducing OpenAI o3 and o4-mini. April 2025. URL https://openai.com/\nindex/introducing-o3-and-o4-mini/ . Accessed: 2025-06-08.\n[52] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\nAdvances in neural information processing systems , 35:27730\u201327744, 2022.\n[53] Parkinson, J. and Cutts, Q. Investigating the relationship between spatial skills and computer\nscience. In Proceedings of the 2018 ACM Conference on International Computing Education\nResearch , pp. 106\u2013114, 2018.\n[54] Presson, C. C., DeLange, N., and Hazelrigg, M. D. Orientation-specificity in kinesthetic spatial\nlearning: The role of multiple orientations. Memory & Cognition , 15(3):225\u2013229, 1987.\n[55] Proske, U. and Gandevia, S. C. The kinaesthetic senses. The Journal of physiology , 587(17):\n4139\u20134146, 2009.\n12\n\n--- Page 13 ---\n[56] Ramakrishnan, S. K., Wijmans, E., Kr\u00e4henb\u00fchl, P., and Koltun, V . Does spatial cognition\nemerge in frontier models? In Proceedings of the International Conference on Learning\nRepresentations (ICLR) , 2025. Poster.\n[57] Ray, A. and others. Sat: Spatial aptitude training for multimodal language models. arXiv\npreprint arXiv:2408.01234 , 2024.\n[58] Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T. P., Alayrac, J., Soricut,\nR., Lazaridou, A., Firat, O., Schrittwieser, J., Antonoglou, I., Anil, R., Borgeaud, S., Dai,\nA. M., Millican, K., Dyer, E., Glaese, M., Sottiaux, T., Lee, B., Viola, F., Reynolds, M.,\nXu, Y ., Molloy, J., Chen, J., Isard, M., Barham, P., Hennigan, T., McIlroy, R., Johnson,\nM., Schalkwyk, J., Collins, E., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Meyer,\nC., Thornton, G., Yang, Z., Michalewski, H., Abbas, Z., Schucher, N., Anand, A., Ives, R.,\nKeeling, J., Lenc, K., Haykal, S., Shakeri, S., Shyam, P., Chowdhery, A., Ring, R., Spencer,\nS., Sezener, E., and et al. Gemini 1.5: Unlocking multimodal understanding across millions\nof tokens of context. CoRR , abs/2403.05530, 2024. doi: 10.48550/ARXIV .2403.05530. URL\nhttps://doi.org/10.48550/arXiv.2403.05530 .\n[59] Shepard, R. N. and Metzler, J. Mental rotation of three-dimensional objects. Science , 171\n(3972):701\u2013703, 1971. doi: 10.1126/science.171.3972.701.\n[60] Shiri, F., Guo, X.-Y ., Far, M. G., Yu, X., Haffari, G., and Li, Y .-F. An empirical analysis on\nspatial reasoning capabilities of large multimodal models. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pp. 21440\u201321455, 2024.\n[61] Stogiannidis, I., McDonagh, S., and Tsaftaris, S. A. Mind the gap: Benchmarking spatial\nreasoning in vision\u2013language models. arXiv preprint arXiv:2503.19707 , 2025. Under review.\n[62] Suhr, A., Zhou, Y ., Zhang, Z., Bai, H., Cho, K., and Artzi, Y . A corpus for reasoning about\nnatural language grounded in photographs. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL) , pp. 6418\u20136428, 2019.\n[63] Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova,\nT., Ram\u00e9, A., Rivi\u00e8re, M., et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786 ,\n2025.\n[64] Uttal, D. H., Meadow, N. G., Tipton, E., Hand, L. L., Alden, A. R., Warren, L., and Newcombe,\nN. S. The malleability of spatial skills: A meta-analysis of training studies. Psychological\nBulletin , 139(2):352\u2013402, 2013. doi: 10.1037/a0028446.\n[65] Uttal, D. H., McKee, K., Simms, N., Hegarty, M., and Newcombe, N. S. How can we best\nassess spatial skills? practical and conceptual challenges. Journal of Intelligence , 12(1):8, 2024.\n[66] Wai, J., Lubinski, D., and Benbow, C. P. Spatial ability for stem domains: Aligning over 50\nyears of cumulative psychological knowledge solidifies its importance. Journal of Educational\nPsychology , 101:817\u2013835, 2009. URL https://api.semanticscholar.org/CorpusID:\n17233758 .\n[67] Wang, J., Ming, Y ., Shi, Z., Vineet, V ., Wang, X., Li, Y ., and Joshi, N. Is a picture worth a\nthousand words? delving into spatial reasoning for vision\u2013language models. arXiv preprint\narXiv:2409.12345 , 2024.\n[68] Wang, K., Pan, J., Shi, W., Lu, Z., Ren, H., Zhou, A., Zhan, M., and Li, H. Measuring\nmultimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference\non Neural Information Processing Systems Datasets and Benchmarks Track , 2024. URL\nhttps://openreview.net/forum?id=QWTCcxMpPA .\n[69] Wang, K., Pan, J., Wei, L., Zhou, A., Shi, W., Lu, Z., Xiao, H., Yang, Y ., Ren, H., Zhan, M.,\nand Li, H. Mathcoder-VL: Bridging vision and code for enhanced multimodal mathematical\nreasoning. In The 63rd Annual Meeting of the Association for Computational Linguistics , 2025.\nURL https://openreview.net/forum?id=nuvtX1imAb .\n13\n\n--- Page 14 ---\n[70] Wei, E. X., Anson, E. R., Resnick, S. M., and Agrawal, Y . Psychometric tests and spatial\nnavigation: Data from the baltimore longitudinal study of aging. Frontiers in neurology , 11:\n484, 2020.\n[71] Wei, K., Paskov, P., Dev, S., Byun, M. J., Reuel, A., Roberts-Gaal, X., Calcott, R., Coxon, E.,\nand Deshpande, C. Position: Human baselines in model evaluations need rigor and transparency\n(with recommendations & reporting checklist). In Forty-second International Conference on\nMachine Learning Position Paper Track , 2025. URL https://openreview.net/forum?id=\ngwhPvu97Gm .\n[72] Xu, W., Lyu, D., Wang, W., Feng, J., Gao, C., and Li, Y . Defining and evaluating visual\nlanguage models\u2019 basic spatial abilities: A perspective from psychometrics. arXiv preprint\narXiv:2502.11859 , 2025.\n[73] Xu, Y ., Li, C., Zhou, H., Wan, X., Zhang, C., Korhonen, A., and Vuli \u00b4c, I. Visual planning: Let\u2019s\nthink only with images. arXiv preprint arXiv:2505.11409 , 2025.\n[74] Yang, J., Yang, S., Gupta, A. W., Han, R., Fei-Fei, L., and Xie, S. Thinking in space:\nHow multimodal large language models see, remember, and recall spaces. arXiv preprint\narXiv:2412.14171 , 2024.\n[75] Yang, K., Russakovsky, O., and Deng, J. Spatialsense: An adversarially crowdsourced bench-\nmark for spatial relation recognition. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pp. 2041\u20132050, 2019.\n[76] Yang, Y ., Yih, W.-t., and Meek, C. WikiQA: A challenge dataset for open-domain question\nanswering. In M\u00e0rquez, L., Callison-Burch, C., and Su, J. (eds.), Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing , pp. 2013\u20132018, Lisbon,\nPortugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/\nD15-1237. URL https://aclanthology.org/D15-1237/ .\n[77] Yi, K., Gan, C., Li, Y ., Wu, J., Kushman, N., Tenenbaum, J. B., and Kohli, P. Clevrer: Collision\nevents for video representation and reasoning. In International Conference on Learning\nRepresentations (ICLR) , 2020.\n[78] Y\u0131lmaz, H. B. On the development and measurement of spatial ability. International Electronic\nJournal of Elementary Education , 1(2):83\u201396, 2009.\n[79] Zhang, H., Li, C., Wu, W., Mao, S., Zhang, Y ., Tian, H., Vuli \u00b4c, I., Zhang, Z., Wang, L., Tan, T.,\net al. Scaling and beyond: Advancing spatial reasoning in mllms requires new recipes. arXiv\npreprint arXiv:2504.15037 , 2025.\n[80] Zhang, Y ., Zhang, H., Tian, H., Fu, C., Zhang, S., Wu, J., Li, F., Wang, K., Wen, Q., Zhang,\nZ., et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world\nscenarios that are difficult for humans? In The Thirteenth International Conference on Learning\nRepresentations .\n[81] Zhou, L., Moreno-Casares, P. A., Mart\u00ednez-Plumed, F., Burden, J., Burnell, R., Cheke, L., Ferri,\nC., Marcoci, A., Mehrbakhsh, B., Moros-Daval, Y ., et al. Predictable artificial intelligence.\narXiv preprint arXiv:2310.06167 , 2023.\n[82] Zhou, L., Pacchiardi, L., Mart\u00ednez-Plumed, F., Collins, K. M., Moros-Daval, Y ., Zhang, S.,\nZhao, Q., Huang, Y ., Sun, L., Prunty, J. E., et al. General scales unlock ai evaluation with\nexplanatory and predictive power. arXiv preprint arXiv:2503.06378 , 2025.\n[83] Zhou, Q., Wang, Z., Rimfeld, K., Allegrini, A. G., Plomin, R., and Malanchini, M. Exploring\nthe specific predictive ability of multiple domains of spatial ability on stem educational out-\ncomes. bioRxiv , 2024. doi: 10.1101/2024.12.20.629833. URL https://www.biorxiv.org/\ncontent/early/2024/12/22/2024.12.20.629833 .\n14\n\n--- Page 15 ---\nA 11P LUS-BENCH\nOverview of the Framework We introduce an evaluation framework designed for a fine-grained\nanalysis of MLLMs\u2019 spatial reasoning capabilities. The framework extends beyond previous bench-\nmarks in three crucial ways.\n1. Disentangling Cognitive Features (\u00a73.2). Previous benchmarks often conflate distinct cognitive\nfeatures that affect model accuracy in spatial reasoning tasks, such as perceptual difficulty and inherent\nreasoning complexity. Ignoring these features undermines evaluation validity and explainability,\nhindering real-world applicability when selecting appropriate models [6]. Our framework explicitly\nidentifies and accounts for these performance-affecting features:\n\u2022Visual Perception : Complex visual patterns require accurate interpretation of pattern structures\nbefore reasoning begins.\n\u2022General Reasoning : The inherent complexity of the reasoning process itself, e.g., requiring\nmultiple reasoning hops or intricate spatial transformations, adds difficulty that might overshadow\nan MLLM\u2019s genuine spatial reasoning capabilities.\n2. Instance-Wise Evaluation with Predictive Power (\u00a74.2). Typical average-based benchmark scores\n(e.g., accuracy) primarily represent overall performance, making it difficult to anticipate whether a\nmodel will correctly answer a new question. Inspired by Zhou et al. [82], our framework enhances\ninterpretability by supporting instance-wise evaluation. This allows researchers to estimate the\nlikelihood that a model will correctly answer a given question based on known cognitive features [ 6],\ninforming both deployment decisions and future research directions.\n3. Parallel Analysis with Human Cognitive Profiles (\u00a74.2). Despite drawing inspiration from human\ncognitive tests, previous work lacks direct comparison with human cognition. We bridge this gap by\nincorporating human evaluation with response time for each question as a proxy for human-perceived\ntask difficulty [ 4,35]. This parallel analysis reveals the extent to which current MLLMs emulate or\ndiverge from human-like spatial cognition, offering insights to guide the advancement of MLLMs.\nThis dataset is for research purposes only and should not be used outside of research contexts.\nData Source We construct the benchmark from two primary sources: a public subset collected\nfrom the web and a private subset sourced from purchased educational materials. For the public data,\nwe crawl the web using carefully selected spatial reasoning keywords. For the private dataset, we\nacquire spatial aptitude test materials from certified test preparation providers, targeting children\nunder 11 years old.\nTo ensure the quality of the crawled data and retain only well-formed spatial problems, we implement\na filtering pipeline that discard repetitive items based on the urls and ask human annotators to filter out\nsamples that are irrelevant, ambiguous or do not evaluate spatial reasoning. All the data is expressed\nin English.\nTargeted Capabilities and Task Types We focus on spatial cognition tasks designed for young\nadolescents, using the 11+ exam level as an anchor. Given that not all spatial cognitive skills are\nequally suited for evaluation in MLLMs, we concentrate on the following three core capabilities:\nSpatial Relation and Orientation ,Spatial Visualization andFlexibility of Closure . Each capability\nencompasses a collection of tasks, with definitions and examples summarized in Table 1. The selected\ntasks emphasize interpretable reasoning steps and perceptual challenges amenable to MLLM analysis.\nExpert Annotation Protocol We recruit three domain experts to annotate the benchmark data. All\nannotators hold postgraduate degrees or higher in STEM fields, with backgrounds in mathematics\nor engineering. The annotation process adheres to institutional ethical guidelines. All annotations\nare collected anonymously and no information that names or uniquely identifies individual people\nor offensive content are collected or used. The instructions explain that the data would be used for\nresearch purpose only.\nAnnotation Fields and Guidelines As described in Section 3.2, all samples are annotated for two\ncognitive dimensions: Visual Perception Complexity andGeneral Reasoning Process .\n15\n\n--- Page 16 ---\nTable 1: Spatial capabilities and corresponding tasks, with question descriptions and number of\nexamples in public and private split.\nCapability Task Question Description Public Private\nSpatial Relation\nand Orientation2D shape\nrotation\n(SRO.1)The image shows several 2D shapes,\nincluding a designated target shape. Select\nthe option that is the target shape rotated to\na different orientation.35 10\n2D shape\nreflection\n(SRO.2)The image displays several 2D shapes, with\none identified as the target shape. The target\nshape has been reflected across a mirror line\nshown in the image.33 -\n3D shape\nrotation\n(SRO.3)This image shows a 3D polycube shape.\nChoose the option that represents the same\nshape, viewed from a different rotation.6 3\nSpatial Visual-\nizationShape com-\npletion\n(SV .1)The image presents an equation involving a\ntarget shape and several shape candidates\nthat can be added to or removed from the\nbase shape.9 10\nShape com-\nbination\n(SV .2)The image illustrates an equation involving\na basic shape, where shapes are either added\nor removed. Only edges labeled with the\nsame letter can be combined.68 10\nBuilding\nblocks\n(SV .3)The image displays a target complex 3D\nshape along with several sets of blocks.\nIdentify the set of blocks that can be\ncombined to form the target shape.52 10\nPaper fold-\ning (SV .4)The image shows a piece of paper being\nfolded and then punched with holes. Select\nthe option that correctly shows the pattern\nof holes after the paper is fully unfolded.229 9\nCube and\nnets (SV .5)The image shows an unfolded shape (net)\nand several cube candidates. Identify which\noption can be correctly folded into a cube\nfrom the given unfolded shape.201 9\nFlexibility of\nClosureHidden\nshape (FoC)The target shape is hidden within one of the\nanswer options. It may be rotated and\nembedded within the option. Identify the\noption that contains the hidden target shape.76 10\nComprehensive\n(SV+SRO)Cube and\ndice (Com.1)The image shows different views of the\nsame cube, with a unique symbol on each of\nits six faces. Determine which option\ncorrectly matches the missing face.17 10\n3D-2D view\n(Com.2)This image displays a 3D object. Select the\noption that correctly represents a 2D view of\nthe object from a specific perspective.98 10\n16\n\n--- Page 17 ---\nFor tasks with highly standardized visual transformations, such as 2D shape rotation, 2D shape\nreflection, or 3D-2D view, we do not require annotators to document full reasoning steps, as these\nprocesses are straightforward and consistent across samples. For all other tasks, each expert indepen-\ndently provides both visual perception and reasoning annotations according to the detailed protocol\ndescribed below.\nVisual Perception Complexity We quantify visual complexity for both the question and the option\nchoices. The complexity score is derived from the number of atomic components in each pattern. We\ndefine atomic components as key features:\n\u2022 For referable shapes (e.g., heart, star), complexity is based on the number of symbolic elements.\n\u2022 For abstract 2D patterns, we count the number of lines or segments.\n\u2022 For abstract 3D structures, we count the number of surfaces or faces.\nThis methodology yields a consistent, interpretable complexity score for each visual input. Example\nannotations are shown in Figure 2.\nGeneral Reasoning Process To capture the reasoning process, we define a taxonomy of atomic\noperations that cover a wide range of spatial reasoning strategies. Annotators must select one\noperation per step from the categories defined below:\nPattern Matching : Determine whether one entity visually contains or resembles another. The match\ncan be based on exact visual similarity or shared key features. Shape matching does not involve\nreasoning about spatial relationships, nor does it alter the spatial properties of the entities involved.\ndef pattern_match(entity_a: Object, entity_b: Object) -> bool\nSpatial Relation Analysis : Analyze the spatial relationship between two entities. Any two non-\noverlapping 2D or 3D shapes can be treated as separate entities, for example, two cubes, or two faces\nof the same cube, depending on the context of analysis. This process does not change the spatial\nproperties or the overall spatial layout of the entities. Subtypes include:\n\u2022 Position: Determine the relative position of shape B within entity A.\n\u2022Orientation: Determine the direction a part of shape A or entity B is facing (e.g., \"Part X of\nA points toward C\").\n\u2022 Perspective: Infer the viewpoint (e.g., \"viewed from behind\").\n\u2022 Rotation: Determine the direction or angle of rotation.\n\u2022 Folding: Determine the direction in which a 2D net folds into a 3D object.\n\u2022 Projection: Determine the direction in which a 3D entity is projected onto a 2D plane.\ndef spatial_relation(entity_a: Object, entity_b: Object) -> statement: str\nSpatial Manipulation : Change the spatial properties or overall spatial layout of entities.\n\u2022 2D operations: rotation, translation, reflection, adding/removing shapes\n\u2022 3D operations: 3D rotation (around an axis), 3D translation, 3D symmetry\n\u2022 Dimensional transformations: projection in a certain direction, folding along an edge\n\u2022 Counting: e.g., counting the number of holes in an origami structure\n\u2022 Symbol tagging: labeling shapes or parts with markers or symbols\ndef spatial_manipulate(entity: Object, statement: str) -> Union[Object, str]\nLogical Deduction : Infer rules or verify spatial conditions.\n\u2022 Logical inference: inferring spatial properties or rules, such as:\n\u2013\"A cannot be adjacent to B\"\n\u2013\"A must be opposite to C\"\n17\n\n--- Page 18 ---\nFigure 5: Data distributions over lengths of reasoning process and golden options.\nTable 2: Prompt templates for main experiments with single image as input.\nSingle Image Input\n<QUESTION>\n<image>\nConclude your chosen answer to the multiple-choice question between <ANSWER> and\n</ANSWER>.\n\u2013\"Cube A can be obtained from Cube B via one or two rotations\"\n\u2022 Verification: testing whether a property or rule holds on another entity\ndef logical_deduction(*statements: str) -> Union[str, bool]\nAnnotators are instructed to decompose their reasoning into step-by-step sequences using these\noperations, ensuring consistency and reproducibility. This structured representation enables us to\nmap human reasoning steps to potential model behaviors.\nB Experiments\nB.1 Human Participants\nWe recruit three human participants as evaluators to evaluate human performance and record human\nbehavior (response time in seconds). They are not involved in the annotation process with STEM\nmajor background for bachelors major, such as Informatics and AI. All the human evaluators are\ngathered physically to conduct human evaluations, making sure that the performance really reflects\ntheir abilities and behaviors.\nB.2 Models\nHyperparameters We adopt most of the inference parameters by default for proprietary models.\nFor open-sourced models, we adopt the default configuration in HuggingFace.\nPrompts Table 2 and 3 show the prompt templates for single image setting and separate image\nsetting respectively. Within the prompt templates, <QUESTION> and <OPTIONS> are replaced\nwith the questions in Table 1 for different tasks.\nB.3 Results\nA detailed breakdown of scores per model and per task category is provided in Table 4 and 5 for\nmultiple separate images and single image as input.\n18\n\n--- Page 19 ---\nTable 3: Prompt templates for main experiments with separate image segments as input.\nSeparate Image Input\n<QUESTION>\n<image>\nA: <image>\nB: <image>\nC: <image>\nD: <image>\nE: <image>\nConclude your chosen answer to the multiple-choice question between <ANSWER> and\n</ANSWER>.\nTable 4: Task-wise performance per model with separate multiple images as input.\nModel SRO.1 SRO.2 SRO.3 SV .1 SV .2 SV .3 SV .4 SV .5 FoC Com.1 Com.2\nGPT 4o 0.267 0.485 0.444 0.158 0.128 0.290 0.357 0.257 0.279 0.222 0.370\nGPT 4.1-mini 0.289 0.273 0.333 0.368 0.295 0.194 0.340 0.248 0.279 0.074 0.278\nGPT 4.1-nano 0.200 0.394 0.444 0.211 0.192 0.387 0.269 0.195 0.163 0.185 0.269\nGPT-o1 0.378 0.364 0.444 0.158 0.205 0.258 0.445 0.338 0.256 0.222 0.324\nGPT-o3 0.444 0.485 0.556 0.316 0.295 0.274 0.458 0.448 0.349 0.185 0.306\nGPT-o4-mini 0.267 0.485 0.444 0.263 0.231 0.452 0.395 0.305 0.349 0.185 0.306\nGemini 2.0 Flash 0.222 0.212 0.444 0.158 0.179 0.323 0.382 0.257 0.267 0.185 0.278\nGemini 2.5 Flash 0.356 0.242 0.444 0.211 0.269 0.339 0.395 0.276 0.174 0.296 0.315\nGemini 2.5 Pro 0.333 0.394 0.222 0.263 0.308 0.323 0.378 0.300 0.128 0.296 0.324\nOpen-Sourced Models\nQwen 2.5VL 3B 0.267 0.182 0.333 0.158 0.295 0.387 0.235 0.276 0.198 0.259 0.278\nQwen 2.5VL 7B 0.133 0.424 0.111 0.211 0.218 0.387 0.218 0.214 0.209 0.407 0.306\nGemma3 12B 0.289 0.212 0.333 0.316 0.154 0.242 0.265 0.205 0.209 0.185 0.157\nGemma3 27B 0.178 0.303 0.333 0.211 0.192 0.258 0.324 0.238 0.128 0.259 0.231\nTable 5: Task-wise performance per model with single images as input.\nModel SRO.1 SRO.2 SRO.3 SV .1 SV .2 SV .3 SV .4 SV .5 FoC Com.1 Com.2\nGPT 4o 0.156 0.364 0.333 0.368 0.256 0.371 0.248 0.224 0.244 0.407 0.231\nGPT 4.1-mini 0.111 0.242 0.556 0.211 0.167 0.290 0.265 0.214 0.291 0.185 0.250\nGPT 4.1-nano 0.267 0.303 0.556 0.105 0.218 0.323 0.311 0.186 0.221 0.185 0.130\nGPT-o1 0.200 0.364 0.444 0.211 0.167 0.242 0.261 0.238 0.233 0.185 0.250\nGPT-o3 0.378 0.273 0.444 0.158 0.282 0.306 0.382 0.400 0.221 0.148 0.231\nGPT-o4-mini 0.311 0.394 0.222 0.211 0.218 0.339 0.332 0.300 0.291 0.148 0.278\nGemini 2.0 Flash 0.178 0.242 0.333 0.211 0.128 0.435 0.298 0.276 0.256 0.111 0.204\nGemini 2.5 Flash 0.178 0.242 0.333 0.211 0.128 0.435 0.298 0.276 0.256 0.111 0.204\nGemini 2.5 Pro 0.267 0.333 0.333 0.263 0.205 0.323 0.387 0.410 0.279 0.296 0.259\nOpen-Sourced Models\nQwen 2.5VL 3B 0.267 0.212 0.222 0.211 0.269 0.194 0.227 0.276 0.279 0.185 0.176\nQwen 2.5VL 7B 0.333 0.212 0.111 0.211 0.321 0.435 0.235 0.229 0.174 0.111 0.250\nGemma3 12B 0.156 0.212 0.222 0.316 0.282 0.371 0.231 0.229 0.256 0.370 0.241\nGemma3 27B 0.200 0.212 0.111 0.211 0.244 0.274 0.227 0.224 0.221 0.111 0.139\n19\n\n--- Page 20 ---\nTable 6: Response format parsing result with single image as input.\nModel Success Ordinal Number Letter UnknownVerbalized Parsing\nChoice Failure\nGPT 4o 843 10 34 - 27 - 1\nGPT 4.1-mini 846 14 43 3 9 - -\nGPT 4.1-nano 801 3 50 16 25 20\nGPT-o1 852 - 31 3 27 1 1\nGPT-o3 862 1 34 1 16 - 1\nGPT-o4-mini 818 - 33 5 21 34 3\nGPT 4.1 855 6 30 - 24 - -\nGemini 2.0 Flash 728 12 19 4 23 129 -\nGemini 2.5 Flash\nGemini 2.5 Pro\nQwen 2.5VL 3B 814 - 22 15 48 13 3\nQwen 2.5VL 7B 829 - 26 18 39 3 -\nGemma3 12B 824 1 55 1 11 22 1\nGemma3 27B 817 5 44 10 37 1 1\nTable 7: Response format parsing result with separate multiple images as inputs.\nModel Success Ordinal Number Letter UnknownVerbalized Parsing\nChoice Failure\nGPT 4o 901 - 7 - 2 1 4\nGPT 4.1-mini 900 - 12 2 - - 1\nGPT 4.1-nano 866 - 18 13 6 12 -\nGPT-o1 903 - 7 2 2 - 1\nGPT-o3 910 - 2 1 - 1 1\nGPT-o4-mini 863 - 11 1 - 38 2\nGPT 4.1 898 - 13 1 3 - -\nGemini 2.0 Flash 642 - - - - 273 -\nGemini 2.5 Flash - - - - - 909 5\nGemini 2.5 Pro\nQwen 2.5VL 3B 752 - 5 11 28 119 -\nQwen 2.5VL 7B 848 - 5 39 22 1 -\nGemma3 12B 881 - 1 3 - 28 2\nGemma3 27B 903 - 6 - 2 4 -\nFigure 6 and 7 present extended cognitive pattern analyses across individual human participants and\na broader set of MLLM variants. For human participants, Pattern Complexity consistently ranks as\nthe most influential factor for correctness, while Logical Deduction andPattern Matching appear\nless impactful. Moreover, reasoning-related features contribute most significantly to response time,\nwhereas perceptual features such as Pattern Complexity andImage Resolution are among the least\ninfluential in determining response time per sample.\nIn contrast, classifiers trained to predict MLLM correctness do not significantly outperform a random\nbaseline, as indicated by the orange highlights in Figure 7. No consistent cognitive profiles emerge\nacross model variants: different features dominate in different models, suggesting a lack of stable,\ninterpretable reasoning strategies in current MLLMs.\n20\n\n--- Page 21 ---\nHuman Evaluator 1\nCorrectness: \n289 / 402 = 0.719\nHuman Evaluator 2\nCorrectness: \n349 / 402 = 0.868Random Forest over Correctness\nHuman Evaluator 3\nCorrectness: \n341 / 402 = 0.848\nLinear Regression over Response Time\nAccuracy:0.846\nF1 Score:0.821 AUC:0.643Train Loss:     0.128 +- 0.018\nTest Loss:      0.177 +- 4.040\nAccuracy:0.816\nF1 Score:0.799 AUC: 0.621Train Loss:     0.141 +- 0.019\nTest Loss:      0.192 +- 6.073\nAggregated Results\nCorrectness 3/3: 241 / 402\nCorrectness 2/3: 115 / 402\nCorrectness 1/3 or None: 46 / 402Accuracy:0.542\nF1 Score:0.279\nTrain Loss:     0.263 +- 0.028\nTest Loss:      0.144 +- 4.746\nTree Analysis \n- SHAP ValueAccuracy:0.652\nF1 Score:0.631 AUC: 0.579Train Loss:     0.279 +- 0.024\nTest Loss:      0.063 +- 5.222Regression Coefficient \nAnalysis Figure 6: Feature Relevance in the Cognitive Profiles of Individual Human Participants and Aggre-\ngated Human Behavior. Individual human responses are predictable with p < 0.0002 for F1 score\ncompared to random chance.\n21\n\n--- Page 22 ---\nGPT-o1\nCorrectness: \n128 / 402 = 0.318GPT-o3\nCorrectness: \n155 / 402 = 0.386Random Forest over Model Response Correctness\nGPT o4-mini\nCorrectness: \n130 / 402 = 0.323\nLinear Regression over Response Length (# Tokens)    Accuracy:    0.632\n    F1 Score:    0.615 (p=0.0138)\n    AUC:        0.521\nGemini 2.5 Pro\nCorrectness: \n118 / 402 = 0.294Random Forest over Model Response Correctness\nGemini 2.5 Flash\nCorrectness:  \n128 / 402 = 0.318\n    Accuracy:    0.555\n    F1 Score:    0.549 (p=0.0156)\n    AUC:       0.542\n    Accuracy:    0.644\n    F1 Score:    0.623 (p=0.00140)\n    AUC:        0.579\n    Accuracy:    0.619\n    F1 Score:    0.599 (p=0.0636)\n    AUC:        0.531\n    Accuracy:    0.647\n    F1 Score:     0.629 (p=0.000200)\n    AUC:         0.495Train Loss:     0.193 +- 0.015\nTest Loss:      -17.693 +- 576.340Figure 7: Feature Relevance in the Cognitive Profiles of Different Model Variants.\n22",
  "project_dir": "artifacts/projects/11PlusBench_MLLM_Spatial_Reasoning_Evaluator",
  "communication_dir": "artifacts/projects/11PlusBench_MLLM_Spatial_Reasoning_Evaluator/.agent_comm",
  "assigned_at": "2025-08-28T20:44:21.844441",
  "status": "assigned"
}